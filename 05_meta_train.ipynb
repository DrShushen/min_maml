{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as disp\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import RandomSampler, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchviz import make_dot\n",
    "\n",
    "from data import TrainTestSplitter, CurveTasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty print.\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device.\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Randomness.\n",
    "np.random.seed(5)\n",
    "torch.manual_seed(5)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.L1 = nn.Linear(1, 10)\n",
    "        self.L2 = nn.Linear(10, 10)\n",
    "        self.L3 = nn.Linear(10, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = nn.Sigmoid()(self.L1(x))\n",
    "        h2 = nn.Sigmoid()(self.L2(h1))\n",
    "        out = self.L3(h2)\n",
    "        return out\n",
    "\n",
    "class FastModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, meta_model):\n",
    "        \n",
    "        super(FastModel, self).__init__()\n",
    "        \n",
    "        # Clone:\n",
    "        self.meta_model = meta_model\n",
    "        \n",
    "        self.L1w = self.meta_model.L1.weight.clone()\n",
    "        self.L1b = self.meta_model.L1.bias.clone()\n",
    "        \n",
    "        self.L2w = self.meta_model.L2.weight.clone()\n",
    "        self.L2b = self.meta_model.L2.bias.clone()\n",
    "        \n",
    "        self.L3w = self.meta_model.L3.weight.clone()\n",
    "        self.L3b = self.meta_model.L3.bias.clone()\n",
    "        \n",
    "        # Freeze the meta weights:\n",
    "        for name, param in self.meta_model.named_parameters():\n",
    "            param.requires_grad_(False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = nn.Sigmoid()(F.linear(x, self.L1w, self.L1b))\n",
    "        h2 = nn.Sigmoid()(F.linear(h1, self.L2w, self.L2b))\n",
    "        out = F.linear(h1, self.L3w, self.L3b)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y, y_pred):\n",
    "    return (y_pred - y)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_initialisation(model, gain=1.0, torch_seed=42):\n",
    "    torch.manual_seed(torch_seed)\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            torch.nn.init.xavier_uniform_(param, gain=gain)\n",
    "        elif \"bias\" in name:\n",
    "            torch.nn.init.ones_(param)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown model parameter '{}' (not 'weight' or 'bias') found.\".format(name))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_model_initialisation(model, from_model):\n",
    "    parlist = sorted(list(model.named_parameters()))\n",
    "    from_parlist = sorted(list(from_model.named_parameters()))\n",
    "    for idx, (name, param) in enumerate(from_parlist):\n",
    "        assert parlist[idx][0] == name, \"Parameter mismatch.\"\n",
    "        parlist[idx][1] = param\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_model_initialisation_clone(model, from_model):\n",
    "    parlist = sorted(list(model.named_parameters()))\n",
    "    from_parlist = sorted(list(from_model.named_parameters()))\n",
    "    for idx, (name, param) in enumerate(from_parlist):\n",
    "        assert parlist[idx][0] == name, \"Parameter mismatch.\"\n",
    "        parlist[idx][1].data = param.clone()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data.\n",
    "tts = TrainTestSplitter(test_frac=0.4)\n",
    "meta_train = CurveTasks(train_test_splitter=tts, meta_train=True)\n",
    "meta_test = CurveTasks(train_test_splitter=tts, meta_train=False)\n",
    "dl_meta_train = DataLoader(meta_train, sampler=RandomSampler(meta_train, replacement=False))\n",
    "dl_meta_test = DataLoader(meta_test, sampler=RandomSampler(meta_test, replacement=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "META:\n",
      "[   (   'L1.weight',\n",
      "        Parameter containing:\n",
      "tensor([[ 0.7645],\n",
      "        [ 0.8300],\n",
      "        [-0.2343],\n",
      "        [ 0.9186],\n",
      "        [-0.2191],\n",
      "        [ 0.2018],\n",
      "        [-0.4869],\n",
      "        [ 0.5873],\n",
      "        [ 0.8815],\n",
      "        [-0.7336]])),\n",
      "    (   'L1.bias',\n",
      "        Parameter containing:\n",
      "tensor([ 0.8692,  0.1872,  0.7388,  0.1354,  0.4822, -0.1412,  0.7709,  0.1478,\n",
      "        -0.4668,  0.2549])),\n",
      "    (   'L2.weight',\n",
      "        Parameter containing:\n",
      "tensor([[-0.1457, -0.0371, -0.1284,  0.2098, -0.2496, -0.1458, -0.0893, -0.1901,\n",
      "          0.0298, -0.3123],\n",
      "        [ 0.2856, -0.2686,  0.2441,  0.0526, -0.1027,  0.1954,  0.0493,  0.2555,\n",
      "          0.0346, -0.0997],\n",
      "        [ 0.0850, -0.0858,  0.1331,  0.2823,  0.1828, -0.1382,  0.1825,  0.0566,\n",
      "          0.1606, -0.1927],\n",
      "        [-0.3130, -0.1222, -0.2426,  0.2595,  0.0911,  0.1310,  0.1000, -0.0055,\n",
      "          0.2475, -0.2247],\n",
      "        [ 0.0199, -0.2158,  0.0975, -0.1089,  0.0969, -0.0659,  0.2623, -0.1874,\n",
      "         -0.1886, -0.1886],\n",
      "        [ 0.2844,  0.1054,  0.3043, -0.2610, -0.3137, -0.2474, -0.2127,  0.1281,\n",
      "          0.1132,  0.2628],\n",
      "        [-0.1633, -0.2156,  0.1678, -0.1278,  0.1919, -0.0750,  0.1809, -0.2457,\n",
      "         -0.1596,  0.0964],\n",
      "        [ 0.0669, -0.0806,  0.1885,  0.2150, -0.2293, -0.1688,  0.2896, -0.1067,\n",
      "         -0.1121, -0.3060],\n",
      "        [-0.1811,  0.0790, -0.0417, -0.2295,  0.0074, -0.2160, -0.2683, -0.1741,\n",
      "         -0.2768, -0.2014],\n",
      "        [ 0.3161,  0.0597,  0.0974, -0.2949, -0.2077, -0.1053,  0.0494, -0.2783,\n",
      "         -0.1363, -0.1893]])),\n",
      "    (   'L2.bias',\n",
      "        Parameter containing:\n",
      "tensor([ 0.0009, -0.1177, -0.0219, -0.2143, -0.2171, -0.1845, -0.1082, -0.2496,\n",
      "         0.2651, -0.0628])),\n",
      "    (   'L3.weight',\n",
      "        Parameter containing:\n",
      "tensor([[ 0.2721,  0.0985, -0.2678,  0.2188, -0.0870, -0.1212, -0.2625, -0.3144,\n",
      "          0.0905, -0.0691]])),\n",
      "    ('L3.bias', Parameter containing:\n",
      "tensor([0.1231]))]\n",
      "FAST:\n",
      "[   (   'meta_model.L1.weight',\n",
      "        Parameter containing:\n",
      "tensor([[ 0.7645],\n",
      "        [ 0.8300],\n",
      "        [-0.2343],\n",
      "        [ 0.9186],\n",
      "        [-0.2191],\n",
      "        [ 0.2018],\n",
      "        [-0.4869],\n",
      "        [ 0.5873],\n",
      "        [ 0.8815],\n",
      "        [-0.7336]])),\n",
      "    (   'meta_model.L1.bias',\n",
      "        Parameter containing:\n",
      "tensor([ 0.8692,  0.1872,  0.7388,  0.1354,  0.4822, -0.1412,  0.7709,  0.1478,\n",
      "        -0.4668,  0.2549])),\n",
      "    (   'meta_model.L2.weight',\n",
      "        Parameter containing:\n",
      "tensor([[-0.1457, -0.0371, -0.1284,  0.2098, -0.2496, -0.1458, -0.0893, -0.1901,\n",
      "          0.0298, -0.3123],\n",
      "        [ 0.2856, -0.2686,  0.2441,  0.0526, -0.1027,  0.1954,  0.0493,  0.2555,\n",
      "          0.0346, -0.0997],\n",
      "        [ 0.0850, -0.0858,  0.1331,  0.2823,  0.1828, -0.1382,  0.1825,  0.0566,\n",
      "          0.1606, -0.1927],\n",
      "        [-0.3130, -0.1222, -0.2426,  0.2595,  0.0911,  0.1310,  0.1000, -0.0055,\n",
      "          0.2475, -0.2247],\n",
      "        [ 0.0199, -0.2158,  0.0975, -0.1089,  0.0969, -0.0659,  0.2623, -0.1874,\n",
      "         -0.1886, -0.1886],\n",
      "        [ 0.2844,  0.1054,  0.3043, -0.2610, -0.3137, -0.2474, -0.2127,  0.1281,\n",
      "          0.1132,  0.2628],\n",
      "        [-0.1633, -0.2156,  0.1678, -0.1278,  0.1919, -0.0750,  0.1809, -0.2457,\n",
      "         -0.1596,  0.0964],\n",
      "        [ 0.0669, -0.0806,  0.1885,  0.2150, -0.2293, -0.1688,  0.2896, -0.1067,\n",
      "         -0.1121, -0.3060],\n",
      "        [-0.1811,  0.0790, -0.0417, -0.2295,  0.0074, -0.2160, -0.2683, -0.1741,\n",
      "         -0.2768, -0.2014],\n",
      "        [ 0.3161,  0.0597,  0.0974, -0.2949, -0.2077, -0.1053,  0.0494, -0.2783,\n",
      "         -0.1363, -0.1893]])),\n",
      "    (   'meta_model.L2.bias',\n",
      "        Parameter containing:\n",
      "tensor([ 0.0009, -0.1177, -0.0219, -0.2143, -0.2171, -0.1845, -0.1082, -0.2496,\n",
      "         0.2651, -0.0628])),\n",
      "    (   'meta_model.L3.weight',\n",
      "        Parameter containing:\n",
      "tensor([[ 0.2721,  0.0985, -0.2678,  0.2188, -0.0870, -0.1212, -0.2625, -0.3144,\n",
      "          0.0905, -0.0691]])),\n",
      "    ('meta_model.L3.bias', Parameter containing:\n",
      "tensor([0.1231]))]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)  # For parameter initialisation.\n",
    "meta_model = Model()\n",
    "fast_model = FastModel(meta_model)\n",
    "\n",
    "# TODO: WORKING HERE...\n",
    "\n",
    "print(\"META:\")\n",
    "pp.pprint(list(meta_model.named_parameters()))\n",
    "\n",
    "print(\"FAST:\")\n",
    "pp.pprint(list(fast_model.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance after 1 gradient step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop.\n",
    "\n",
    "def training_loop(model, x_train, y_train, x_test, y_test, device, epochs=1, use_manual_sgd=False, display=False, retain_graph=False, create_graph=False):\n",
    "    \n",
    "    model = model.to(device)\n",
    "    x_train = x_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    x_test = x_test.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "    \n",
    "    lr = 0.1\n",
    "\n",
    "    x = x_train[0]\n",
    "    y = y_train[0]\n",
    "\n",
    "    x_test_ = x_test[0]\n",
    "    y_test_ = y_test[0]\n",
    "\n",
    "    if use_manual_sgd is False:\n",
    "        opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0)\n",
    "\n",
    "    epoch_losses_train = np.zeros((epochs,))\n",
    "    epoch_losses_test = np.zeros((epochs,))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        if display:\n",
    "            disp.clear_output(wait=True)\n",
    "            print(\"------------------------------\\nepoch {}:\\n------------------------------\\n\".format(epoch + 1))\n",
    "\n",
    "        #######\n",
    "        # Train\n",
    "        #######\n",
    "\n",
    "        model.train()  # Trian mode.\n",
    "\n",
    "        y_preds = torch.zeros(len(x), requires_grad=False)\n",
    "        mse_losses = torch.zeros(len(x), requires_grad=False)\n",
    "\n",
    "        for it in range(len(x)):\n",
    "            # Zero the gradients.\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Predict.\n",
    "            x_it = x[it].float().unsqueeze(0)\n",
    "            y_pred = model(x_it)\n",
    "            y_preds[it] = y_pred.detach()\n",
    "\n",
    "            # Compute loss.\n",
    "            mse_loss = compute_mse(y_pred.squeeze(), y[it])\n",
    "            mse_losses[it] = mse_loss\n",
    "\n",
    "            if use_manual_sgd is False:\n",
    "\n",
    "                # Backprop.\n",
    "                mse_loss.backward(torch.tensor(1.).to(device))\n",
    "\n",
    "                # Gradient descent.\n",
    "                opt.step()\n",
    "\n",
    "            else:\n",
    "\n",
    "                params_list = list(model.parameters())\n",
    "                grads = torch.autograd.grad(outputs=mse_loss, inputs=params_list, grad_outputs=torch.tensor(1.).to(device), retain_graph=retain_graph, create_graph=create_graph)\n",
    "                for idx, parameter in enumerate(params_list):\n",
    "                    parameter.grad = grads[idx].data\n",
    "\n",
    "                with torch.no_grad():  # Do not track.\n",
    "                    for name, parameter in model.named_parameters():\n",
    "                        parameter.data -= lr * parameter.grad.data\n",
    "\n",
    "        ##########\n",
    "        # Evaluate\n",
    "        ##########\n",
    "\n",
    "        model.eval()  # Evaluation mode.\n",
    "\n",
    "        # Test loss:\n",
    "        y_preds_test = torch.zeros(len(x_test_), requires_grad=False)\n",
    "        mse_losses_test = torch.zeros(len(x_test_), requires_grad=False)\n",
    "        for it in range(len(x_test_)):\n",
    "            # Compute loss.\n",
    "            x_it_test = x_test_[it].float().unsqueeze(0)\n",
    "            y_pred_test = model(x_it_test)\n",
    "            y_preds_test[it] = y_pred_test.detach()\n",
    "            mse_loss_test = compute_mse(y_pred_test.squeeze(), y_test_[it])\n",
    "            mse_losses_test[it] = mse_loss_test\n",
    "        \n",
    "        if display:\n",
    "            f1 = show_plot(x, y, y_preds, x_test_, y_test_, y_preds_test, ylim=None)\n",
    "\n",
    "        epoch_mse_train = mse_losses.sum()\n",
    "        epoch_mse_test = mse_losses_test.sum()\n",
    "\n",
    "        epoch_losses_train[epoch] = epoch_mse_train.item()\n",
    "        epoch_losses_test[epoch] = epoch_mse_test.item()\n",
    "        if display:\n",
    "            f2 = plot_losses(epoch_losses_train[:epoch+1], epoch_losses_test[:epoch+1], xlim=(0, epochs-1))\n",
    "\n",
    "        #########\n",
    "        # Display\n",
    "        #########\n",
    "        \n",
    "        if display:\n",
    "            disp.display(f1)\n",
    "            disp.display(f2)\n",
    "            print()\n",
    "            print(\"Training loss: {}\".format(epoch_mse_train))\n",
    "            print(\"Test loss: {}\".format(epoch_mse_test))\n",
    "        \n",
    "    return epoch_mse_train, epoch_mse_test, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_evaluate(data_loader, model, param_init, param_init_kwargs, use_manual_sgd=False):\n",
    "    \n",
    "    df = pd.DataFrame(columns=(\"training_MSE\", \"test_MSE\"), index=[0])\n",
    "    \n",
    "    torch.manual_seed(41)  # For data loader shuffling.\n",
    "    for idx, ((x_train, y_train), (x_test, y_test)) in enumerate(data_loader):\n",
    "        \n",
    "        if \"torch_seed\" in param_init_kwargs:  # To vary random seed over meta test datasets.\n",
    "            param_init_kwargs[\"torch_seed\"] += 1\n",
    "        \n",
    "        model = param_init(model, **param_init_kwargs)\n",
    "        \n",
    "        tr, ts, _ = training_loop(model, x_train, y_train, x_test, y_test, device, epochs=1, use_manual_sgd=False, display=False, retain_graph=False, create_graph=False)\n",
    "        df.loc[idx, (\"training_MSE\", \"test_MSE\")] = tr.item(), ts.item()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training_MSE    141.533891\n",
       "test_MSE         59.671710\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate performance after 1 gradient step.\n",
    "model = Model()\n",
    "\n",
    "df = meta_evaluate(dl_meta_test, model, random_initialisation, {\"gain\": 1.0, \"torch_seed\": 41}, use_manual_sgd=True)\n",
    "\n",
    "disp.display(df.mean())\n",
    "# disp.display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-22b353982d22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# TODO: Now need to properly deal with the \"One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.\" situation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mparams_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msum_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mparameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37_min_maml/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    155\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    156\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         inputs, allow_unused)\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior."
     ]
    }
   ],
   "source": [
    "meta_model = Model()\n",
    "meta_model = meta_model.to(device)\n",
    "\n",
    "torch.manual_seed(42)  # For parameter initialisation.\n",
    "meta_model = random_initialisation(meta_model)  # Theta.\n",
    "\n",
    "loss_track = []\n",
    "model_track = []\n",
    "for idx, ((x_train, y_train), (x_test, y_test)) in enumerate(dl_meta_train):\n",
    "    fast_model = Model()\n",
    "    fast_model = from_model_initialisation(model=fast_model, from_model=meta_model)\n",
    "    tr, ts, fast_model = training_loop(fast_model, x_train, y_train, x_test, y_test, device, epochs=1, use_manual_sgd=True, display=False, retain_graph=True, create_graph=True)\n",
    "#     print(\"FAST:\")\n",
    "#     pp.pprint(list(fast_model.named_parameters()))\n",
    "#     print(\"\\nMETA:\")\n",
    "#     pp.pprint(list(meta_model.named_parameters()))\n",
    "    loss_track.append(tr)\n",
    "    model_track.append(fast_model)\n",
    "\n",
    "sum_losses = torch.stack(loss_track).sum().to(device)\n",
    "\n",
    "### WORKING HERE:\n",
    "# TODO: Now need to properly deal with the \"One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.\" situation.\n",
    "params_list = list(meta_model.parameters())\n",
    "grads = torch.autograd.grad(outputs=sum_losses, inputs=params_list, grad_outputs=torch.tensor(1.).to(device), retain_graph=False, create_graph=False)\n",
    "for idx, parameter in enumerate(params_list):\n",
    "    parameter.grad = grads[idx].data\n",
    "with torch.no_grad():  # Do not track.\n",
    "    for name, parameter in model.named_parameters():\n",
    "        parameter.data -= lr * parameter.grad.data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37_min_maml] *",
   "language": "python",
   "name": "conda-env-py37_min_maml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
