{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as disp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import RandomSampler, DataLoader\n",
    "\n",
    "from data import TrainTestSplitter, CurveTasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device.\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.L1 = nn.Linear(1, 10)\n",
    "        self.L2 = nn.Linear(10, 10)\n",
    "        self.L3 = nn.Linear(10, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = nn.Sigmoid()(self.L1(x))\n",
    "        h2 = nn.Sigmoid()(self.L2(h1))\n",
    "        out = self.L3(h2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y, y_pred):\n",
    "    return (y_pred - y)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data.\n",
    "tts = TrainTestSplitter(test_frac=0.4)\n",
    "meta_train = CurveTasks(train_test_splitter=tts, meta_train=True)\n",
    "meta_test = CurveTasks(train_test_splitter=tts, meta_train=False)\n",
    "dl_train = DataLoader(meta_train, sampler=RandomSampler(meta_train, replacement=False))\n",
    "dl_test = DataLoader(meta_test, sampler=RandomSampler(meta_test, replacement=False))\n",
    "\n",
    "torch.manual_seed(5)  # Set seed for splitting.\n",
    "for ((x_train, y_train), (x_test, y_test)) in dl_train:\n",
    "    break  # Just take the first meta dataset.\n",
    "    \n",
    "x_train = x_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "x_test = x_test.to(device)\n",
    "y_test = y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot(x, y, y_preds, x_test, y_test, y_preds_test, ylim=None):\n",
    "    \n",
    "    x_ = x.cpu().numpy()\n",
    "    y_ = y.cpu().numpy()\n",
    "    y_preds_ = y_preds.cpu().numpy()\n",
    "    x_test_ = x_test.cpu().numpy()\n",
    "    y_test_ = y_test.cpu().numpy()\n",
    "    y_preds_test_ = y_preds_test.cpu().numpy()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.scatter(x_, y_, color=\"red\", label=\"train target\")\n",
    "    plt.scatter(x_, y_preds_, color=\"blue\", label=\"train prediction\")\n",
    "    plt.scatter(x_test_, y_test_, color=\"pink\", label=\"test target\")\n",
    "    plt.scatter(x_test_, y_preds_test_, color=\"lightblue\", label=\"test prediction\")\n",
    "    plt.ylim(ylim)\n",
    "    plt.legend()\n",
    "    plt.title(\"Prediction\")\n",
    "    \n",
    "    f = plt.gcf()\n",
    "    plt.close()\n",
    "    \n",
    "    return f\n",
    "\n",
    "def plot_losses(train_loss, test_loss, xlim=None):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(train_loss, label=\"training loss\")\n",
    "    plt.plot(test_loss, label=\"test loss\")\n",
    "    plt.xlim(xlim)\n",
    "    plt.legend()\n",
    "    plt.title(\"Losses\")\n",
    "    \n",
    "    f = plt.gcf()\n",
    "    plt.close()\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use manual version of SGD?\n",
    "use_manual_sgd = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimise.\n",
    "\n",
    "torch.manual_seed(9)  # Set seed for optimisation.\n",
    "model = Model().to(device)\n",
    "\n",
    "epochs = 100\n",
    "lr = 0.1\n",
    "\n",
    "x = x_train[0]\n",
    "y = y_train[0]\n",
    "\n",
    "x_test_ = x_test[0]\n",
    "y_test_ = y_test[0]\n",
    "\n",
    "opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0)\n",
    "\n",
    "epoch_losses_train = np.zeros((epochs,))\n",
    "epoch_losses_test = np.zeros((epochs,))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    disp.clear_output(wait=True)\n",
    "    print(\"------------------------------\\nepoch {}:\\n------------------------------\\n\".format(epoch + 1))\n",
    "    \n",
    "    #######\n",
    "    # Train\n",
    "    #######\n",
    "    \n",
    "    model.train()  # Trian mode.\n",
    "    \n",
    "    y_preds = torch.zeros(len(x), requires_grad=False)\n",
    "    mse_losses = torch.zeros(len(x), requires_grad=False)\n",
    "    \n",
    "    for it in range(len(x)):\n",
    "        # Zero the gradients.\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Predict.\n",
    "        x_it = x[it].float().unsqueeze(0)\n",
    "        y_pred = model(x_it)\n",
    "        y_preds[it] = y_pred.detach()\n",
    "        \n",
    "        # Compute loss.\n",
    "        mse_loss = compute_mse(y_pred.squeeze(), y[it])\n",
    "        mse_losses[it] = mse_loss\n",
    "        \n",
    "        if use_manual_sgd is not True:\n",
    "\n",
    "            # Backprop.\n",
    "            mse_loss.backward(torch.tensor(1.).to(device))\n",
    "            \n",
    "            # Gradient descent.\n",
    "            opt.step()\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            params_list = list(model.parameters())\n",
    "            grads = torch.autograd.grad(outputs=mse_loss, inputs=params_list, grad_outputs=torch.tensor(1.).to(device), retain_graph=False, create_graph=False)\n",
    "            for idx, parameter in enumerate(params_list):\n",
    "                parameter.grad = grads[idx].data\n",
    "            \n",
    "            with torch.no_grad():  # Do not track.\n",
    "                for name, parameter in model.named_parameters():\n",
    "                    parameter.data -= lr * parameter.grad.data\n",
    "    \n",
    "    ##########\n",
    "    # Evaluate\n",
    "    ##########\n",
    "    \n",
    "    model.eval()  # Evaluation mode.\n",
    "    \n",
    "    # Test loss:\n",
    "    y_preds_test = torch.zeros(len(x_test_), requires_grad=False)\n",
    "    mse_losses_test = torch.zeros(len(x_test_), requires_grad=False)\n",
    "    for it in range(len(x_test_)):\n",
    "        # Compute loss.\n",
    "        x_it_test = x_test_[it].float().unsqueeze(0)\n",
    "        y_pred_test = model(x_it_test)\n",
    "        y_preds_test[it] = y_pred_test.detach()\n",
    "        mse_loss_test = compute_mse(y_pred_test.squeeze(), y_test_[it])\n",
    "        mse_losses_test[it] = mse_loss_test\n",
    "    \n",
    "    f1 = show_plot(x, y, y_preds, x_test_, y_test_, y_preds_test, ylim=None)\n",
    "    \n",
    "    epoch_mse_train = mse_losses.sum().item()\n",
    "    epoch_mse_test = mse_losses_test.sum().item()\n",
    "    \n",
    "    epoch_losses_train[epoch] = epoch_mse_train\n",
    "    epoch_losses_test[epoch] = epoch_mse_test\n",
    "    f2 = plot_losses(epoch_losses_train[:epoch+1], epoch_losses_test[:epoch+1], xlim=(0, epochs-1))\n",
    "    \n",
    "    #########\n",
    "    # Display\n",
    "    #########\n",
    "    \n",
    "    disp.display(f1)\n",
    "    disp.display(f2)\n",
    "    print()\n",
    "    print(\"Training loss: {}\".format(epoch_mse_train))\n",
    "    print(\"Test loss: {}\".format(epoch_mse_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37_min_maml] *",
   "language": "python",
   "name": "conda-env-py37_min_maml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
