{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.autograd\n",
    "\n",
    "from torchviz import make_dot  # https://github.com/szagoruyko/pytorchviz\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import IPython.display as disp\n",
    "\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tinfo(tensor, name=None):\n",
    "    if name is not None:\n",
    "        print(\"{}:\".format(name))\n",
    "        print(\"------------------\")\n",
    "    disp.display(tensor.shape)\n",
    "    disp.display(tensor.dtype)\n",
    "    disp.display(tensor.device)\n",
    "    print(\"------------------\")\n",
    "    disp.display(tensor)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple computation graph example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensors:\n",
    "# Note: wrapped in nn.Parameter(...) only because that allows printing the paramter name on the computational graph, not necessary for computation.\n",
    "x = nn.Parameter(torch.tensor([[1, 2, 3, 4, 5]], dtype=float, requires_grad=True))\n",
    "W = nn.Parameter(torch.tensor(np.array([[10, 10], [20, 20], [30, 30], [40, 40], [50, 50]]), dtype=float, requires_grad=True))\n",
    "b = nn.Parameter(torch.tensor([1.2, 1.2], dtype=float, requires_grad=True))\n",
    "\n",
    "tinfo(x, \"x\")\n",
    "tinfo(W, \"W\")\n",
    "tinfo(b, \"b\")\n",
    "\n",
    "# Define computations:\n",
    "xW = torch.mm(x, W)\n",
    "z = xW + b\n",
    "o = torch.log(z)\n",
    "\n",
    "tinfo(xW, \"xW = torch.mm(x, W)\")\n",
    "tinfo(z, \"z = xW + b\")\n",
    "tinfo(o, \"o = torch.log(z)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name the parameters\n",
    "params = {\"x\": x, \"W\": W, \"b\": b}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display graph:\n",
    "make_dot(o, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Example\n",
    "* From https://github.com/szagoruyko/pytorchviz/blob/master/examples.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential()\n",
    "model.add_module('W0', nn.Linear(8, 16))\n",
    "model.add_module('tanh', nn.Tanh())\n",
    "model.add_module('W1', nn.Linear(16, 1))\n",
    "\n",
    "x = torch.randn(1,8)\n",
    "\n",
    "params_dict = dict(model.named_parameters())\n",
    "pp.pprint(params_dict)\n",
    "\n",
    "make_dot(model(x), params=params_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data:\n",
    "x_np = np.linspace(0, 2 * np.pi, 10)\n",
    "y_np = np.sin(x_np)\n",
    "disp.display(x_np, y_np)\n",
    "plt.scatter(x_np, y_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model.\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.W1 = nn.Linear(10, 10)\n",
    "        self.b = nn.Parameter(torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=torch.float32, requires_grad=True))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.W1(x) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss.\n",
    "def mse(y, t):\n",
    "    return torch.mean((y - t)**2)\n",
    "def mse_np(y, t):\n",
    "    return np.mean((y - t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show computation graph.\n",
    "\n",
    "x = torch.tensor([list(x_np)], dtype=torch.float32, requires_grad=False)  # requires_grad=True not needed?\n",
    "y = torch.tensor([list(y_np)], dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "torch.manual_seed(9)\n",
    "model = Model()\n",
    "\n",
    "params_dict = dict(list(model.named_parameters()) + [(\"x\", x)])\n",
    "pp.pprint(params_dict)\n",
    "\n",
    "make_dot(model(x), params=params_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial \"prediction\".\n",
    "\n",
    "y_pred = model(x).squeeze().detach().numpy()\n",
    "\n",
    "def show_plot(x, y, y_pred, ylim=(-1., 1.), print_mse=True):\n",
    "    # print(y, y_pred)\n",
    "    plt.cla()\n",
    "    plt.scatter(x, y, color=\"red\")\n",
    "    plt.scatter(x, y_pred, color=\"blue\")\n",
    "    plt.ylim(ylim)\n",
    "    if print_mse:\n",
    "        print(\"MSE = {}\".format(mse_np(y_pred, y)))\n",
    "\n",
    "show_plot(x_np, y_np, y_pred, ylim=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement GD.\n",
    "\n",
    "VERB = False\n",
    "use_manual_gd = True\n",
    "\n",
    "torch.manual_seed(9)\n",
    "model = Model()\n",
    "\n",
    "iters = 20\n",
    "lr = 0.01\n",
    "\n",
    "opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0)\n",
    "\n",
    "for it in range(iters):\n",
    "    \n",
    "    print(\"------------------------------\\nit {}:\\n------------------------------\\n\".format(it + 1))\n",
    "    \n",
    "    # Zero the gradients.\n",
    "    with torch.no_grad():\n",
    "        for name, parameter in model.named_parameters():\n",
    "            if parameter.grad is not None:\n",
    "                parameter.grad.data.zero_()\n",
    "    \n",
    "    for name, parameter in model.named_parameters():\n",
    "        if VERB:\n",
    "            if name == \"b\":\n",
    "                print(\"\\n{}:\\n---\\n\".format(name))\n",
    "                print(\"AT THE START\")\n",
    "                print(parameter)\n",
    "                print()\n",
    "    \n",
    "    # Compute loss.\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    print(\">>> y_pred:\")\n",
    "    print(y_pred)\n",
    "    \n",
    "    print(\">>> y:\")\n",
    "    print(y)\n",
    "    \n",
    "    mse_loss = mse(y_pred, y)\n",
    "    \n",
    "    print(\">>> mse = {}\".format(mse_loss))\n",
    "    \n",
    "    # Backprop.\n",
    "    mse_loss.backward(torch.tensor(1.))\n",
    "    \n",
    "    # Gradient descent.\n",
    "    if use_manual_gd is False:\n",
    "        for name, parameter in model.named_parameters():\n",
    "            if VERB:\n",
    "                if name == \"b\":\n",
    "                    print(\"\\n{}:\\n---\\n\".format(name))\n",
    "                    print(\"BEFORE\")\n",
    "                    print(parameter)\n",
    "                    print(\"GRAD\")\n",
    "                    print(parameter.grad)\n",
    "        opt.step()\n",
    "        for name, parameter in model.named_parameters():\n",
    "            if VERB:\n",
    "                if name == \"b\":\n",
    "                    print(\"\\n{}:\\n---\\n\".format(name))\n",
    "                    print(\"AFTER\")\n",
    "                    print(parameter)\n",
    "    \n",
    "    if use_manual_gd is True:\n",
    "        with torch.no_grad():  # Do not track.\n",
    "            for name, parameter in model.named_parameters():\n",
    "                if VERB:\n",
    "                    if name == \"b\":\n",
    "                        print(\"\\n{}:\\n---\\n\".format(name))\n",
    "                        print(\"BEFORE\")\n",
    "                        print(parameter)\n",
    "                        print(\"GRAD\")\n",
    "                        print(parameter.grad)\n",
    "                parameter.data -= lr * parameter.grad.data\n",
    "                if VERB:\n",
    "                    if name == \"b\":\n",
    "                        print(\"AFTER\")\n",
    "                        print(parameter)\n",
    "    \n",
    "    # Display.\n",
    "    show_plot(x_np, y_np, y_pred.squeeze().detach().numpy(), ylim=(-4., 4.), print_mse=False)\n",
    "    if it < iters-1:\n",
    "        disp.clear_output(wait=True)\n",
    "        disp.display(plt.gcf())\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate graph node state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(vars_tuple_list):\n",
    "    df = pd.DataFrame(columns=(\"name\", \"requires_grad\", \"is_leaf\", \"requires_grad & is_leaf\", \"grad\", \"grad_fn\"), index=list(range(len(vars_tuple_list))))\n",
    "    for idx, (name, var) in enumerate(vars_tuple_list):\n",
    "        df.loc[idx, \"name\"] = name\n",
    "        df.loc[idx, \"requires_grad\"] = var.requires_grad\n",
    "        df.loc[idx, \"is_leaf\"] = var.is_leaf\n",
    "        df.loc[idx, \"requires_grad & is_leaf\"] = var.requires_grad & var.is_leaf\n",
    "        df.loc[idx, \"grad\"] = str(var.grad)\n",
    "        df.loc[idx, \"grad_fn\"] = str(var.grad_fn)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(9)\n",
    "model = Model()\n",
    "\n",
    "y_pred = model(x)\n",
    "mse_loss = mse(y_pred, y)\n",
    "\n",
    "params_list = [(\"x\", x)] + list(model.named_parameters()) + [(\"y_pred\", y_pred), (\"y\", y), (\"mse_loss\", mse_loss)]\n",
    "\n",
    "# Graph.\n",
    "print(\"\\nCalculation Graph, all the way to the loss:\")\n",
    "chart = make_dot(mse_loss, params=dict(params_list))\n",
    "disp.display(chart)\n",
    "\n",
    "# Graph node info.\n",
    "print(\"\\nGraph nodes (BEFORE backprop):\")\n",
    "df = show_state(params_list)\n",
    "disp.display(df)\n",
    "\n",
    "# --------------------------\n",
    "# Do one run of backprop.\n",
    "mse_loss.backward()\n",
    "\n",
    "# Graph node info.\n",
    "print(\"\\nGraph nodes (AFTER backprop):\")\n",
    "df = show_state(params_list)\n",
    "disp.display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate `retain_graph`\n",
    "\n",
    "torch.manual_seed(9)\n",
    "model = Model()\n",
    "\n",
    "y_pred = model(x)\n",
    "mse_loss = mse(y_pred, y)\n",
    "\n",
    "params_list = [(\"x\", x)] + list(model.named_parameters()) + [(\"y_pred\", y_pred), (\"y\", y), (\"mse_loss\", mse_loss)]\n",
    "\n",
    "# Graph node info.\n",
    "print(\"\\nGraph nodes (BEFORE backprop):\")\n",
    "df = show_state(params_list)\n",
    "disp.display(df)\n",
    "\n",
    "# --------------------------\n",
    "# Do TWO RUNS of backprop.\n",
    "mse_loss.backward(retain_graph=True)\n",
    "mse_loss.backward()\n",
    "\n",
    "# The below causes an exception:\n",
    "# mse_loss.backward()\n",
    "# mse_loss.backward()\n",
    "\n",
    "# All the above does is compute 1st order derivative twice and accumulate.\n",
    "# NOT 2nd order derivatives.\n",
    "\n",
    "# Graph node info.\n",
    "print(\"\\nGraph nodes (AFTER backprop):\")\n",
    "df = show_state(params_list)\n",
    "disp.display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate `create_graph`\n",
    "\n",
    "torch.manual_seed(9)\n",
    "model = Model()\n",
    "\n",
    "y_pred = model(x)\n",
    "mse_loss = mse(y_pred, y)\n",
    "\n",
    "params_list = [(\"x\", x)] + list(model.named_parameters()) + [(\"y_pred\", y_pred), (\"y\", y), (\"mse_loss\", mse_loss)]\n",
    "\n",
    "# Graph.\n",
    "print(\"\\n`mse_loss` graph:\")\n",
    "chart = make_dot(mse_loss, params=dict(params_list))\n",
    "disp.display(chart)\n",
    "\n",
    "# --------------------------\n",
    "# Do one run of backprop.\n",
    "mse_loss.backward(retain_graph=True, create_graph=True)\n",
    "\n",
    "# Can see that there's now a grath for model.W1.weight.**grad**\n",
    "# Without create_graph=True, there is no graph there.\n",
    "print(\"\\n`model.W1.weight.grad` graph (after `backward()` with `create_graph=True`):\")\n",
    "W1w_grad = model.W1.weight.grad\n",
    "chart = make_dot(W1w_grad)\n",
    "disp.display(chart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `torch.autograd.grad()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually calculate and update gradients using autograd.\n",
    "\n",
    "# Use `torch.autograd.grad()`.\n",
    "# `torch.autograd.backward()` can be used as equivalent to calling `.backward()` on the loss.\n",
    "\n",
    "VERB = False\n",
    "\n",
    "torch.manual_seed(9)\n",
    "model = Model()\n",
    "\n",
    "x.requires_grad_(True)\n",
    "\n",
    "iters = 20\n",
    "lr = 0.01\n",
    "\n",
    "for it in range(iters):\n",
    "    \n",
    "    print(\"------------------------------\\nit {}:\\n------------------------------\\n\".format(it + 1))\n",
    "    \n",
    "    # Zero the gradients.\n",
    "    with torch.no_grad():\n",
    "        for name, parameter in model.named_parameters():\n",
    "            if parameter.grad is not None:\n",
    "                parameter.grad.data.zero_()\n",
    "    \n",
    "    # Compute loss.\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    print(\">>> y_pred:\")\n",
    "    print(y_pred)\n",
    "    \n",
    "    print(\">>> y:\")\n",
    "    print(y)\n",
    "    \n",
    "    mse_loss = mse(y_pred, y)\n",
    "    \n",
    "    print(\">>> mse = {}\".format(mse_loss))\n",
    "    \n",
    "    # --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- \n",
    "    # Backprop.\n",
    "    if VERB:\n",
    "        print(\"-------------------\")\n",
    "        print(\"BEFORE UPDATE:\")\n",
    "        for p in model.parameters():\n",
    "            print(\"parameter:\")\n",
    "            print(p)\n",
    "            print(\"parameter.grad:\")\n",
    "            print(p.grad)\n",
    "            print(\"----\")\n",
    "    \n",
    "    params_list = list(model.parameters())\n",
    "    # MANUALLY update the gradients (note that `only_inputs=True` no longer works in new PyTorch versions):\n",
    "    grads = torch.autograd.grad(outputs=mse_loss, inputs=params_list, grad_outputs=torch.tensor(1.), retain_graph=False, create_graph=False)\n",
    "    for idx, parameter in enumerate(params_list):\n",
    "        if parameter.grad is None:  # Only relevant to the 0th iteration.\n",
    "            parameter.grad = grads[idx]\n",
    "        else:\n",
    "            parameter.grad.data = grads[idx]\n",
    "    \n",
    "    if VERB:\n",
    "        print(\"-------------------\")\n",
    "        print(\"AFTER UPDATE:\")\n",
    "        for p in model.parameters():\n",
    "            print(\"parameter:\")\n",
    "            print(p)\n",
    "            print(\"parameter.grad:\")\n",
    "            print(p.grad)\n",
    "            print(\"----\")\n",
    "    # --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- \n",
    "        \n",
    "    with torch.no_grad():  # Do not track.\n",
    "        for name, parameter in model.named_parameters():\n",
    "            parameter.data -= lr * parameter.grad.data\n",
    "    \n",
    "    # Display.\n",
    "    show_plot(x_np, y_np, y_pred.squeeze().detach().numpy(), ylim=(-4., 4.), print_mse=False)\n",
    "    if it < iters-1:\n",
    "        disp.clear_output(wait=True)\n",
    "        disp.display(plt.gcf())\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higher order derivatives "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple 1D function:\n",
    "x = torch.tensor([3.], requires_grad=True)\n",
    "f = torch.exp(x) + 20 * torch.log(x)\n",
    "\n",
    "# Show graph:\n",
    "disp.display(make_dot(f, params={\"x\": x}))\n",
    "\n",
    "print(\"\\n f = {}. Expected: 42.05778...\".format(f.item()))\n",
    "\n",
    "# First order derivative wrt x: df/dx.\n",
    "df_dx = torch.autograd.grad(outputs=f, inputs=x, grad_outputs=None, retain_graph=True, create_graph=True)[0]\n",
    "print(\"\\n df_dx = {}. Expected: 26.75220...\".format(df_dx.item()))\n",
    "\n",
    "# Second order derivative wrt x: d2f/dx2.\n",
    "d2f_dx2 = torch.autograd.grad(outputs=df_dx, inputs=x, grad_outputs=None, retain_graph=False, create_graph=False)[0]\n",
    "print(\"\\n d2f_dx2 = {}. Expected: 17.8633...\".format(d2f_dx2.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38_maml] *",
   "language": "python",
   "name": "conda-env-py38_maml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
