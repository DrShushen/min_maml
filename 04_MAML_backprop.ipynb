{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as disp\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print(text):\n",
    "    print(\"\\n{}\".format(text))\n",
    "    \n",
    "def print_(text):\n",
    "    print(\"{}\\n\".format(text))\n",
    "\n",
    "def _print_(text):\n",
    "    print(\"\\n{}\\n\".format(text))\n",
    "    \n",
    "def print_all(W, b, x, y, t):\n",
    "    variables = (W, b, x, y, t)\n",
    "    names = (\"W\", \"b\", \"x\", \"y\", \"t\")\n",
    "    for idx, v in enumerate(variables):\n",
    "        print(\"{}.data:\\n{}\".format(names[idx], v))\n",
    "        print(\"{}.grad:\\n{}\".format(names[idx], v.grad))\n",
    "\n",
    "def print_all_id(id_, W, b, x, y, t):\n",
    "    variables = (W, b, x, y, t)\n",
    "    names = [\"{}{}\".format(a, id_) for a in (\"W\", \"b\", \"x\", \"y\", \"t\")]\n",
    "    print(\"-- Parameters of fast model {} --\".format(id_))\n",
    "    for idx, v in enumerate(variables):\n",
    "        print(\"{}.data:\\n{}\".format(names[idx], v))\n",
    "        print(\"{}.grad:\\n{}\".format(names[idx], v.grad))\n",
    "    print()\n",
    "\n",
    "def print_wb(W, b):\n",
    "    variables = (W, b)\n",
    "    names = (\"W\", \"b\")\n",
    "    print(\"-- W, b --\")\n",
    "    for idx, v in enumerate(variables):\n",
    "        print(\"{}.data:\\n{}\".format(names[idx], v))\n",
    "        print(\"{}.grad:\\n{}\".format(names[idx], v.grad))\n",
    "    print(\"-- -- --\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the double gradient calculation.\n",
    "\n",
    "CREATE_GRAPH = True  # Toggling this toggles second order calculation's inclusion of gradients.\n",
    "\n",
    "\n",
    "# Define \"model\".\n",
    "\n",
    "W = torch.tensor([[1., 0.5], [-0.8, 1.3]], requires_grad=True)\n",
    "b = torch.tensor([[1., 1.]], requires_grad=True)\n",
    "\n",
    "x = torch.tensor([[4.5, 9.5]], requires_grad=True)\n",
    "\n",
    "y = torch.matmul(x, W) + b\n",
    "t = torch.tensor([[2., 3.]], requires_grad=False)\n",
    "\n",
    "disp.display(make_dot(y, params={\"W\": W, \"b\": b, \"x\": x, \"y\": y}))\n",
    "print_all(W, b, x, y, t)\n",
    "\n",
    "loss = ((t - y)**2).mean()\n",
    "_print(\"loss = {}\".format(loss))\n",
    "\n",
    "\n",
    "\n",
    "_print_(\"------------------------------------------------------------------\")\n",
    "\n",
    "print(\"Calling autograd.grad...\\n\")\n",
    "grads = torch.autograd.grad(outputs=loss, inputs=(W, b, x), grad_outputs=torch.tensor(1.), retain_graph=True, create_graph=CREATE_GRAPH)\n",
    "print_(\"grads calculated: {}\".format(grads))\n",
    "\n",
    "print_(\"Updating W, b...\")\n",
    "W.grad = grads[0]\n",
    "b.grad = grads[1]\n",
    "W = W - 0.1 * W.grad\n",
    "b = b - 0.1 * b.grad\n",
    "\n",
    "print_all(W, b, x, y, t)\n",
    "\n",
    "_print(\"W graph (after updating):\")\n",
    "disp.display(make_dot(W))\n",
    "\n",
    "t2 = torch.tensor([[2.5, 3.5]], requires_grad=False)\n",
    "y2 = torch.matmul(x, W) + b\n",
    "# disp.display(make_dot(y2))\n",
    "loss2 = ((t2 - y2)**2).mean()\n",
    "\n",
    "\n",
    "\n",
    "_print_(\"------------------------------------------------------------------\")\n",
    "\n",
    "print_(\"Calling autograd.grad 2nd time...\")\n",
    "print(\"grads:\")\n",
    "grads = torch.autograd.grad(outputs=loss2, inputs=(W, b, x), grad_outputs=torch.tensor(1.), retain_graph=False, create_graph=False)\n",
    "print_(grads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE_GRAPH = True\n",
    "\n",
    "# Learning rates.\n",
    "lri = 0.001  # Inner.\n",
    "lro = 0.05  # Outer.\n",
    "\n",
    "\n",
    "\n",
    "# Theta parameters:\n",
    "    \n",
    "W = torch.tensor([[1., 0.5], [-0.8, 1.3]], requires_grad=True)\n",
    "b = torch.tensor([[1., 1.]], requires_grad=True)\n",
    "print(\"W, b initially:\")\n",
    "print_wb(W, b)\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Fast model 1.\n",
    "\n",
    "_print_(\"------------------------------------------------------------------\")\n",
    "print_(\"Fast model 1:\")\n",
    "\n",
    "W1 = W.clone()\n",
    "b1 = b.clone()\n",
    "\n",
    "x1 = torch.tensor([[4.5, 9.5]], requires_grad=True)\n",
    "\n",
    "y1 = torch.matmul(x1, W1) + b1\n",
    "t1 = torch.tensor([[2., 3.]], requires_grad=False)\n",
    "\n",
    "print_all_id(1, W1, b1, x1, y1, t1)\n",
    "\n",
    "loss1 = ((t1 - y1)**2).mean()\n",
    "_print_(\"loss1 = {}\".format(loss1))\n",
    "print(\"loss1 graph:\")\n",
    "disp.display(make_dot(loss1, params={\"W\": W, \"b\": b, \"W1\": W1, \"b1\": b1, \"x1\": x1, \"y1\": y1, \"loss1\": loss1}))\n",
    "\n",
    "grads1 = torch.autograd.grad(outputs=loss1, inputs=(W1, b1), grad_outputs=torch.tensor(1.), retain_graph=True, create_graph=CREATE_GRAPH)\n",
    "W1.grad = grads1[0]\n",
    "b1.grad = grads1[1]\n",
    "W1 = W1 - lri * W1.grad\n",
    "b1 = b1 - lri * b1.grad\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Fast model 2.\n",
    "\n",
    "_print_(\"------------------------------------------------------------------\")\n",
    "print_(\"Fast model 2:\")\n",
    "\n",
    "W2 = W.clone()\n",
    "b2 = b.clone()\n",
    "\n",
    "x2 = torch.tensor([[11.1, 22.2]], requires_grad=True)\n",
    "\n",
    "y2 = torch.matmul(x2, W2) + b2\n",
    "t2 = torch.tensor([[4., 7.]], requires_grad=False)\n",
    "\n",
    "print_all_id(2, W2, b2, x2, y2, t2)\n",
    "\n",
    "loss2 = ((t2 - y2)**2).mean()\n",
    "_print_(\"loss2 = {}\".format(loss2))\n",
    "print(\"loss2 graph:\")\n",
    "disp.display(make_dot(loss2, params={\"W\": W, \"b\": b, \"W2\": W2, \"b2\": b2, \"x2\": x2, \"y2\": y2, \"loss2\": loss2}))\n",
    "\n",
    "grads2 = torch.autograd.grad(outputs=loss2, inputs=(W2, b2), grad_outputs=torch.tensor(1.), retain_graph=True, create_graph=CREATE_GRAPH)\n",
    "W2.grad = grads2[0]\n",
    "b2.grad = grads2[1]\n",
    "W2 = W2 - lri * W2.grad\n",
    "b2 = b2 - lri * b2.grad\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Meta updagte.\n",
    "\n",
    "# print_wb(W, b)  # <-- To check these theta parameters are unchanged from their initial values.\n",
    "\n",
    "_print_(\"------------------------------------------------------------------\")\n",
    "print(\"Meta update...\")\n",
    "\n",
    "# Calculate losses using theta-dash (updated) parameters.\n",
    "y1_ = torch.matmul(x1, W1) + b1\n",
    "loss1_ = ((t1 - y1_)**2).mean()\n",
    "_print(\"loss1_ = {}\".format(loss1_))\n",
    "y2_ = torch.matmul(x2, W2) + b2\n",
    "loss2_ = ((t2 - y2_)**2).mean()\n",
    "print_(\"loss2_ = {}\".format(loss2_))\n",
    "\n",
    "sumloss_ = loss1_ + loss2_\n",
    "print_(\"sumloss_ = {}\".format(sumloss_))\n",
    "\n",
    "print(\"sumloss_ graph:\")\n",
    "disp.display(make_dot(sumloss_))\n",
    "\n",
    "grads_meta = torch.autograd.grad(outputs=sumloss_, inputs=(W, b), grad_outputs=torch.tensor(1.), retain_graph=False, create_graph=False)\n",
    "W.grad = grads_meta[0]\n",
    "b.grad = grads_meta[1]\n",
    "W = W - lro * W.grad\n",
    "b = b - lro * b.grad\n",
    "\n",
    "print(\"W, b after update:\")\n",
    "print_wb(W, b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37_PersRec] *",
   "language": "python",
   "name": "conda-env-py37_PersRec-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
