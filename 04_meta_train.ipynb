{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as disp\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import RandomSampler, DataLoader\n",
    "\n",
    "from data import TrainTestSplitter, CurveTasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty print.\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device.\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "np.random.seed(5)\n",
    "torch.manual_seed(5)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.L1 = nn.Linear(1, 10)\n",
    "        self.L2 = nn.Linear(10, 10)\n",
    "        self.L3 = nn.Linear(10, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = nn.Sigmoid()(self.L1(x))\n",
    "        h2 = nn.Sigmoid()(self.L2(h1))\n",
    "        out = self.L3(h2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y, y_pred):\n",
    "    return (y_pred - y)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_initialisation(model, gain=1.0):\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            torch.nn.init.xavier_uniform_(param, gain=gain)\n",
    "        elif \"bias\" in name:\n",
    "            torch.nn.init.ones_(param)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown model parameter '{}' (not 'weight' or 'bias') found.\".format(name))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data.\n",
    "tts = TrainTestSplitter(test_frac=0.4)\n",
    "meta_train = CurveTasks(train_test_splitter=tts, meta_train=True)\n",
    "meta_test = CurveTasks(train_test_splitter=tts, meta_train=False)\n",
    "dl_meta_train = DataLoader(meta_train, sampler=RandomSampler(meta_train, replacement=False))\n",
    "dl_meta_test = DataLoader(meta_test, sampler=RandomSampler(meta_test, replacement=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance after 1 gradient step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop.\n",
    "\n",
    "def training_loop(model, x_train, y_train, x_test, y_test, device, epochs=1, use_manual_sgd=False, display=False):\n",
    "    \n",
    "    model.to(device)\n",
    "    x_train = x_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    x_test = x_test.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "    \n",
    "    lr = 0.1\n",
    "\n",
    "    x = x_train[0]\n",
    "    y = y_train[0]\n",
    "\n",
    "    x_test_ = x_test[0]\n",
    "    y_test_ = y_test[0]\n",
    "\n",
    "    if use_manual_sgd is False:\n",
    "        opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0)\n",
    "\n",
    "    epoch_losses_train = np.zeros((epochs,))\n",
    "    epoch_losses_test = np.zeros((epochs,))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        if display:\n",
    "            disp.clear_output(wait=True)\n",
    "            print(\"------------------------------\\nepoch {}:\\n------------------------------\\n\".format(epoch + 1))\n",
    "\n",
    "        #######\n",
    "        # Train\n",
    "        #######\n",
    "\n",
    "        model.train()  # Trian mode.\n",
    "\n",
    "        y_preds = torch.zeros(len(x), requires_grad=False)\n",
    "        mse_losses = torch.zeros(len(x), requires_grad=False)\n",
    "\n",
    "        for it in range(len(x)):\n",
    "            # Zero the gradients.\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Predict.\n",
    "            x_it = x[it].float().unsqueeze(0)\n",
    "            y_pred = model(x_it)\n",
    "            y_preds[it] = y_pred.detach()\n",
    "\n",
    "            # Compute loss.\n",
    "            mse_loss = compute_mse(y_pred.squeeze(), y[it])\n",
    "            mse_losses[it] = mse_loss\n",
    "\n",
    "            if use_manual_sgd is False:\n",
    "\n",
    "                # Backprop.\n",
    "                mse_loss.backward(torch.tensor(1.).to(device))\n",
    "\n",
    "                # Gradient descent.\n",
    "                opt.step()\n",
    "\n",
    "            else:\n",
    "\n",
    "                params_list = list(model.parameters())\n",
    "                grads = torch.autograd.grad(outputs=mse_loss, inputs=params_list, grad_outputs=torch.tensor(1.).to(device), retain_graph=False, create_graph=False)\n",
    "                for idx, parameter in enumerate(params_list):\n",
    "                    parameter.grad = grads[idx].data\n",
    "\n",
    "                with torch.no_grad():  # Do not track.\n",
    "                    for name, parameter in model.named_parameters():\n",
    "                        parameter.data -= lr * parameter.grad.data\n",
    "\n",
    "        ##########\n",
    "        # Evaluate\n",
    "        ##########\n",
    "\n",
    "        model.eval()  # Evaluation mode.\n",
    "\n",
    "        # Test loss:\n",
    "        y_preds_test = torch.zeros(len(x_test_), requires_grad=False)\n",
    "        mse_losses_test = torch.zeros(len(x_test_), requires_grad=False)\n",
    "        for it in range(len(x_test_)):\n",
    "            # Compute loss.\n",
    "            x_it_test = x_test_[it].float().unsqueeze(0)\n",
    "            y_pred_test = model(x_it_test)\n",
    "            y_preds_test[it] = y_pred_test.detach()\n",
    "            mse_loss_test = compute_mse(y_pred_test.squeeze(), y_test_[it])\n",
    "            mse_losses_test[it] = mse_loss_test\n",
    "        \n",
    "        if display:\n",
    "            f1 = show_plot(x, y, y_preds, x_test_, y_test_, y_preds_test, ylim=None)\n",
    "\n",
    "        epoch_mse_train = mse_losses.sum().item()\n",
    "        epoch_mse_test = mse_losses_test.sum().item()\n",
    "\n",
    "        epoch_losses_train[epoch] = epoch_mse_train\n",
    "        epoch_losses_test[epoch] = epoch_mse_test\n",
    "        if display:\n",
    "            f2 = plot_losses(epoch_losses_train[:epoch+1], epoch_losses_test[:epoch+1], xlim=(0, epochs-1))\n",
    "\n",
    "        #########\n",
    "        # Display\n",
    "        #########\n",
    "        \n",
    "        if display:\n",
    "            disp.display(f1)\n",
    "            disp.display(f2)\n",
    "            print()\n",
    "            print(\"Training loss: {}\".format(epoch_mse_train))\n",
    "            print(\"Test loss: {}\".format(epoch_mse_test))\n",
    "        \n",
    "    return epoch_mse_train, epoch_mse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training_MSE    141.533891\n",
       "test_MSE         59.671710\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate performance after 1 gradient step.\n",
    "model = Model()\n",
    "\n",
    "df = pd.DataFrame(columns=(\"training_MSE\", \"test_MSE\"), index=[0])\n",
    "torch.manual_seed(41)  # For data loader shuffling.\n",
    "for idx, ((x_train, y_train), (x_test, y_test)) in enumerate(dl_meta_test):\n",
    "    torch.manual_seed(42 + idx)  # For parameter initialisation and subsequent training.\n",
    "    model = random_initialisation(model)\n",
    "    tr, ts = training_loop(model, x_train, y_train, x_test, y_test, device, epochs=1, use_manual_sgd=False, display=False)\n",
    "    df.loc[idx, (\"training_MSE\", \"test_MSE\")] = tr, ts\n",
    "\n",
    "disp.display(df.mean())\n",
    "# disp.display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37_min_maml] *",
   "language": "python",
   "name": "conda-env-py37_min_maml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
