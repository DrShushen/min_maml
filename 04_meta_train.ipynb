{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as disp\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import RandomSampler, DataLoader\n",
    "\n",
    "from data import TrainTestSplitter, CurveTasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty print.\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device.\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Randomness.\n",
    "np.random.seed(5)\n",
    "torch.manual_seed(5)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.L1 = nn.Linear(1, 10)\n",
    "        self.L2 = nn.Linear(10, 10)\n",
    "        self.L3 = nn.Linear(10, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = nn.Sigmoid()(self.L1(x))\n",
    "        h2 = nn.Sigmoid()(self.L2(h1))\n",
    "        out = self.L3(h2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y, y_pred):\n",
    "    return (y_pred - y)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_initialisation(model, gain=1.0, torch_seed=42):\n",
    "    torch.manual_seed(torch_seed)\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            torch.nn.init.xavier_uniform_(param, gain=gain)\n",
    "        elif \"bias\" in name:\n",
    "            torch.nn.init.ones_(param)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown model parameter '{}' (not 'weight' or 'bias') found.\".format(name))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_model_initialisation(model, from_model):\n",
    "    parlist = sorted(list(model.named_parameters()))\n",
    "    from_parlist = sorted(list(from_model.named_parameters()))\n",
    "    for idx, (name, param) in enumerate(from_parlist):\n",
    "        assert parlist[idx][0] == name, \"Parameter mismatch.\"\n",
    "        parlist[idx][1].data = param.data\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data.\n",
    "tts = TrainTestSplitter(test_frac=0.4)\n",
    "meta_train = CurveTasks(train_test_splitter=tts, meta_train=True)\n",
    "meta_test = CurveTasks(train_test_splitter=tts, meta_train=False)\n",
    "dl_meta_train = DataLoader(meta_train, sampler=RandomSampler(meta_train, replacement=False))\n",
    "dl_meta_test = DataLoader(meta_test, sampler=RandomSampler(meta_test, replacement=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance after 1 gradient step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop.\n",
    "\n",
    "def training_loop(model, x_train, y_train, x_test, y_test, device, epochs=1, use_manual_sgd=False, display=False, retain_graph=False, create_graph=False):\n",
    "    \n",
    "    model = model.to(device)\n",
    "    x_train = x_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    x_test = x_test.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "    \n",
    "    lr = 0.1\n",
    "\n",
    "    x = x_train[0]\n",
    "    y = y_train[0]\n",
    "\n",
    "    x_test_ = x_test[0]\n",
    "    y_test_ = y_test[0]\n",
    "\n",
    "    if use_manual_sgd is False:\n",
    "        opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0)\n",
    "\n",
    "    epoch_losses_train = np.zeros((epochs,))\n",
    "    epoch_losses_test = np.zeros((epochs,))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        if display:\n",
    "            disp.clear_output(wait=True)\n",
    "            print(\"------------------------------\\nepoch {}:\\n------------------------------\\n\".format(epoch + 1))\n",
    "\n",
    "        #######\n",
    "        # Train\n",
    "        #######\n",
    "\n",
    "        model.train()  # Trian mode.\n",
    "\n",
    "        y_preds = torch.zeros(len(x), requires_grad=False)\n",
    "        mse_losses = torch.zeros(len(x), requires_grad=False)\n",
    "\n",
    "        for it in range(len(x)):\n",
    "            # Zero the gradients.\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Predict.\n",
    "            x_it = x[it].float().unsqueeze(0)\n",
    "            y_pred = model(x_it)\n",
    "            y_preds[it] = y_pred.detach()\n",
    "\n",
    "            # Compute loss.\n",
    "            mse_loss = compute_mse(y_pred.squeeze(), y[it])\n",
    "            mse_losses[it] = mse_loss\n",
    "\n",
    "            if use_manual_sgd is False:\n",
    "\n",
    "                # Backprop.\n",
    "                mse_loss.backward(torch.tensor(1.).to(device))\n",
    "\n",
    "                # Gradient descent.\n",
    "                opt.step()\n",
    "\n",
    "            else:\n",
    "\n",
    "                params_list = list(model.parameters())\n",
    "                grads = torch.autograd.grad(outputs=mse_loss, inputs=params_list, grad_outputs=torch.tensor(1.).to(device), retain_graph=retain_graph, create_graph=create_graph)\n",
    "                for idx, parameter in enumerate(params_list):\n",
    "                    parameter.grad = grads[idx].data\n",
    "\n",
    "                with torch.no_grad():  # Do not track.\n",
    "                    for name, parameter in model.named_parameters():\n",
    "                        parameter.data -= lr * parameter.grad.data\n",
    "\n",
    "        ##########\n",
    "        # Evaluate\n",
    "        ##########\n",
    "\n",
    "        model.eval()  # Evaluation mode.\n",
    "\n",
    "        # Test loss:\n",
    "        y_preds_test = torch.zeros(len(x_test_), requires_grad=False)\n",
    "        mse_losses_test = torch.zeros(len(x_test_), requires_grad=False)\n",
    "        for it in range(len(x_test_)):\n",
    "            # Compute loss.\n",
    "            x_it_test = x_test_[it].float().unsqueeze(0)\n",
    "            y_pred_test = model(x_it_test)\n",
    "            y_preds_test[it] = y_pred_test.detach()\n",
    "            mse_loss_test = compute_mse(y_pred_test.squeeze(), y_test_[it])\n",
    "            mse_losses_test[it] = mse_loss_test\n",
    "        \n",
    "        if display:\n",
    "            f1 = show_plot(x, y, y_preds, x_test_, y_test_, y_preds_test, ylim=None)\n",
    "\n",
    "        epoch_mse_train = mse_losses.sum()\n",
    "        epoch_mse_test = mse_losses_test.sum()\n",
    "\n",
    "        epoch_losses_train[epoch] = epoch_mse_train.item()\n",
    "        epoch_losses_test[epoch] = epoch_mse_test.item()\n",
    "        if display:\n",
    "            f2 = plot_losses(epoch_losses_train[:epoch+1], epoch_losses_test[:epoch+1], xlim=(0, epochs-1))\n",
    "\n",
    "        #########\n",
    "        # Display\n",
    "        #########\n",
    "        \n",
    "        if display:\n",
    "            disp.display(f1)\n",
    "            disp.display(f2)\n",
    "            print()\n",
    "            print(\"Training loss: {}\".format(epoch_mse_train))\n",
    "            print(\"Test loss: {}\".format(epoch_mse_test))\n",
    "        \n",
    "    return epoch_mse_train, epoch_mse_test, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_evaluate(data_loader, model, param_init, param_init_kwargs, use_manual_sgd=False):\n",
    "    \n",
    "    df = pd.DataFrame(columns=(\"training_MSE\", \"test_MSE\"), index=[0])\n",
    "    \n",
    "    torch.manual_seed(41)  # For data loader shuffling.\n",
    "    for idx, ((x_train, y_train), (x_test, y_test)) in enumerate(data_loader):\n",
    "        \n",
    "        if \"torch_seed\" in param_init_kwargs:  # To vary random seed over meta test datasets.\n",
    "            param_init_kwargs[\"torch_seed\"] += 1\n",
    "        \n",
    "        model = param_init(model, **param_init_kwargs)\n",
    "        \n",
    "        tr, ts, _ = training_loop(model, x_train, y_train, x_test, y_test, device, epochs=1, use_manual_sgd=False, display=False, retain_graph=False, create_graph=False)\n",
    "        df.loc[idx, (\"training_MSE\", \"test_MSE\")] = tr.item(), ts.item()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training_MSE    141.533891\n",
       "test_MSE         59.671710\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate performance after 1 gradient step.\n",
    "model = Model()\n",
    "\n",
    "df = meta_evaluate(dl_meta_test, model, random_initialisation, {\"gain\": 1.0, \"torch_seed\": 41}, use_manual_sgd=True)\n",
    "\n",
    "disp.display(df.mean())\n",
    "# disp.display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-29d010222fb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m### WORKING HERE:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mparams_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msum_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mparameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37_min_maml/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    155\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    156\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         inputs, allow_unused)\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior."
     ]
    }
   ],
   "source": [
    "meta_model = Model()\n",
    "meta_model = meta_model.to(device)\n",
    "\n",
    "torch.manual_seed(42)  # For parameter initialisation.\n",
    "meta_model = random_initialisation(meta_model)  # Theta.\n",
    "\n",
    "loss_track = []\n",
    "model_track = []\n",
    "for idx, ((x_train, y_train), (x_test, y_test)) in enumerate(dl_meta_train):\n",
    "    fast_model = Model()\n",
    "    fast_model = from_model_initialisation(model=fast_model, from_model=meta_model)\n",
    "    tr, ts, fast_model = training_loop(fast_model, x_train, y_train, x_test, y_test, device, epochs=1, use_manual_sgd=True, display=False, retain_graph=True, create_graph=True)\n",
    "#     print(\"FAST:\")\n",
    "#     pp.pprint(list(fast_model.named_parameters()))\n",
    "#     print(\"\\nMETA:\")\n",
    "#     pp.pprint(list(meta_model.named_parameters()))\n",
    "    loss_track.append(tr)\n",
    "    model_track.append(fast_model)\n",
    "\n",
    "sum_losses = torch.stack(loss_track).sum().to(device)\n",
    "\n",
    "### WORKING HERE:\n",
    "# TODO: Now need to properly deal with the \"One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.\" situation.\n",
    "params_list = list(meta_model.parameters())\n",
    "grads = torch.autograd.grad(outputs=sum_losses, inputs=params_list, grad_outputs=torch.tensor(1.).to(device), retain_graph=False, create_graph=False)\n",
    "for idx, parameter in enumerate(params_list):\n",
    "    parameter.grad = grads[idx].data\n",
    "with torch.no_grad():  # Do not track.\n",
    "    for name, parameter in model.named_parameters():\n",
    "        parameter.data -= lr * parameter.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Model()\n",
    "\n",
    "# torch.manual_seed(42)  # For parameter initialisation.\n",
    "# model = random_initialisation(model)  # Theta.\n",
    "\n",
    "# model2 = Model()\n",
    "# print(\"BEFORE\")\n",
    "# pp.pprint(list(model2.named_parameters()))\n",
    "\n",
    "# model2 = from_model_initialisation(model2, model)\n",
    "# print(\"AFTER\")\n",
    "# pp.pprint(list(model2.named_parameters()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37_min_maml] *",
   "language": "python",
   "name": "conda-env-py37_min_maml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
