{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as disp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import RandomSampler, DataLoader\n",
    "\n",
    "import higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import TrainTestSplitter, CurveTasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device.\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Randomness.\n",
    "np.random.seed(5)\n",
    "torch.manual_seed(5)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data.\n",
    "tts = TrainTestSplitter(test_frac=0.4)\n",
    "meta_train = CurveTasks(train_test_splitter=tts, meta_train=True)\n",
    "meta_test = CurveTasks(train_test_splitter=tts, meta_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model.\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.L1 = nn.Linear(1, 10)\n",
    "        self.L2 = nn.Linear(10, 10)\n",
    "        self.L3 = nn.Linear(10, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = nn.Sigmoid()(self.L1(x))\n",
    "        h2 = nn.Sigmoid()(self.L2(h1))\n",
    "        out = self.L3(h2)\n",
    "        return out\n",
    "\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y, y_pred):\n",
    "    return (y_pred - y)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta-train:\n",
    "meta_epochs = 10\n",
    "terminate_at_mb = 15\n",
    "print_batch = False\n",
    "random_sampling_order = False\n",
    "\n",
    "if random_sampling_order:\n",
    "    dl_meta_train = DataLoader(meta_train, sampler=RandomSampler(meta_train, replacement=False), batch_size=1)\n",
    "    dl_meta_test = DataLoader(meta_test, sampler=RandomSampler(meta_test, replacement=False), batch_size=1)\n",
    "else:\n",
    "    dl_meta_train = DataLoader(meta_train, batch_size=1)\n",
    "    dl_meta_test = DataLoader(meta_test,  batch_size=1)\n",
    "\n",
    "lr_inner = 0.05\n",
    "lr_outer = 0.05\n",
    "\n",
    "n_iters_inner = 5\n",
    "\n",
    "model = model.float()\n",
    "opt_meta = torch.optim.Adam(model.parameters(), lr=lr_outer)\n",
    "\n",
    "meta_train_losses = []\n",
    "meta_test_losses = []\n",
    "for meta_epoch in range(meta_epochs):\n",
    "    print(f\">> Meta-epoch: {meta_epoch}\")\n",
    "    \n",
    "    me_train_losses = []\n",
    "    me_test_losses = []\n",
    "    \n",
    "    # META-TRAINING:\n",
    "    print(\"META-TRAINING...\")\n",
    "    for mtr_id, ((x_train, y_train), (x_test, y_test)) in enumerate(dl_meta_train):\n",
    "        model.train()\n",
    "        \n",
    "        # Match data dtype to model; set the batch format for the model.\n",
    "        (x_train, y_train), (x_test, y_test) = (x_train.float()[0], y_train.float()[0]), (x_test.float()[0], y_test.float()[0])\n",
    "        \n",
    "        # print(x_train[0]) To check we're sampling same / different items.\n",
    "        \n",
    "        opt_inner = torch.optim.SGD(model.parameters(), lr=lr_inner)\n",
    "        \n",
    "        opt_meta.zero_grad()\n",
    "        \n",
    "        test_losses = []\n",
    "        with higher.innerloop_ctx(model, opt_inner, copy_initial_weights=False) as (functional_model, differentiable_optimiser):\n",
    "            \n",
    "            num_dp_train = len(x_train)\n",
    "            for idx in range(num_dp_train):\n",
    "                # Train fast model:\n",
    "                for _ in range(n_iters_inner):\n",
    "                    pred_fast = functional_model(x_train[idx].unsqueeze(0))\n",
    "                    mse_fast = compute_mse(y_train[idx].squeeze(), pred_fast)\n",
    "                    differentiable_optimiser.step(mse_fast)\n",
    "            \n",
    "            num_dp_test = len(x_test)\n",
    "            for idx in range(num_dp_test):\n",
    "                # Backprop slow model:\n",
    "                pred_slow = functional_model(x_test[idx].unsqueeze(0))\n",
    "                mse_slow = compute_mse(y_test[idx].squeeze(), pred_slow)\n",
    "                test_losses.append(mse_slow.detach())\n",
    "                mse_slow.backward(retain_graph=True)\n",
    "            \n",
    "        opt_meta.step()\n",
    "        test_loss = sum(test_losses) / num_dp_test\n",
    "        \n",
    "        if print_batch:\n",
    "            print(f\"Meta-epoch: {meta_epoch + 1} / Meta-batch: {mtr_id + 1} | Test set loss: {test_loss.item():.5f}\")\n",
    "        me_train_losses.append(test_loss.item())\n",
    "        \n",
    "        if terminate_at_mb is not None and mtr_id == terminate_at_mb:\n",
    "            break\n",
    "    \n",
    "    # META-TESTING:\n",
    "    print(\"META-TESTING...\")\n",
    "    for mtr_id, ((x_train, y_train), (x_test, y_test)) in enumerate(dl_meta_test):\n",
    "        model.eval()\n",
    "        \n",
    "        # Match data dtype to model; set the batch format for the model.\n",
    "        (x_train, y_train), (x_test, y_test) = (x_train.float()[0], y_train.float()[0]), (x_test.float()[0], y_test.float()[0])\n",
    "        \n",
    "        opt_inner = torch.optim.SGD(model.parameters(), lr=lr_inner)\n",
    "        \n",
    "        test_losses = []\n",
    "        with higher.innerloop_ctx(model, opt_inner, copy_initial_weights=False) as (functional_model, differentiable_optimiser):\n",
    "            \n",
    "            num_dp_train = len(x_train)\n",
    "            for idx in range(num_dp_train):\n",
    "                # Train fast model:\n",
    "                for _ in range(n_iters_inner):\n",
    "                    pred_fast = functional_model(x_train[idx].unsqueeze(0))\n",
    "                    mse_fast = compute_mse(y_train[idx].squeeze(), pred_fast)\n",
    "                    differentiable_optimiser.step(mse_fast)\n",
    "            \n",
    "            num_dp_test = len(x_test)\n",
    "            for idx in range(num_dp_test):\n",
    "                # Backprop slow model:\n",
    "                pred_slow = functional_model(x_test[idx].unsqueeze(0)).detach()\n",
    "                mse_slow = compute_mse(y_test[idx].squeeze(), pred_slow)\n",
    "                test_losses.append(mse_slow.detach())\n",
    "        \n",
    "        test_loss = sum(test_losses) / num_dp_test\n",
    "        \n",
    "        if print_batch:\n",
    "            print(f\"Meta-epoch: {meta_epoch + 1} / Meta-batch: {mtr_id + 1} | Test set loss: {test_loss.item():.5f}\")\n",
    "        me_test_losses.append(test_loss.item())\n",
    "        \n",
    "        if terminate_at_mb is not None and mtr_id == terminate_at_mb:\n",
    "            break\n",
    "    \n",
    "    me_train_loss = sum(me_train_losses) / len(me_train_losses)\n",
    "    me_test_loss = sum(me_test_losses) / len(me_test_losses)    \n",
    "    print(f\">> Meta-epoch {meta_epoch}: TR = {me_train_loss:.5f}; TS = {me_test_loss:.5f}\")\n",
    "    \n",
    "    meta_train_losses.append(me_train_loss)\n",
    "    meta_test_losses.append(me_test_loss)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot.\n",
    "\n",
    "def myplot(y1, y2):\n",
    "    x = [str(int(_x)) for _x in list(range(len(y1)))]\n",
    "\n",
    "    plt.plot(x, y1, y2)\n",
    "    plt.title(\"Meta-learning losses over meta-epochs\")\n",
    "    plt.xlabel(\"Meta-epoch\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "\n",
    "    plt.legend([\"Meta-training\", \"Meta-test\"])\n",
    "\n",
    "myplot(meta_train_losses, meta_test_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38_maml] *",
   "language": "python",
   "name": "conda-env-py38_maml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
